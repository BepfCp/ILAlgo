# algo param
algo_name: td3
gamma: 0.99  # discount factor
rho: .005  # target network update rate
c: 0.5  # noise clip threshold for target policy
sigma: 0.2  # std of Gaussian for target actor
expl_std: 0.1  # std of Gaussian for exploration
batch_size: 256  # batch size for both actor and critic
start_timesteps: 25000  # time steps initial random policy is used
eval_freq: 5000  # how often (time steps) we evaluate
max_timesteps: 1000_000  # max time steps to run environment
policy_delay: 2  # frequency of delayed policy updates
buffer_size: 1000_000  # replay buffer size
norm_state: False

# network param
actor_hidden_size: [256, 256]
critic_hidden_size: [256, 256]
actor_lr: !!float 3e-4
critic_lr: !!float 3e-4

# env param
env_name: HalfCheetah-v3
device: "cuda:0"
